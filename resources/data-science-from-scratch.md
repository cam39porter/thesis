# Data Science from Scratch

* *Simpson's Paradox*: correlations can be misleading when confounding variables are ignored

  * Correlation is measuring the relationship between your two variables all else being equal

* Correlation of 0 only means that there is no linear relationship between the variables all else being equal

* Correlation also tells you nothing about how large a relationship is. Variables can be perfectly correlated but tell you nothing

* You can separate correlation from causation with randomized trials (these can be done post fact via random bucketing and simulation)

* Review *Bayesian inference* (pg. 89)

* We use *Principal Component Analysis* when most of the variation in the data seems to be along a single dimension that does not correspond to either the x-axis or the y-axis. PCA is used to capture as much of the variation in the data as possible (pg. 135)

* *k Nearest Neighbors* can help you make predictions, but not understand the model

* Mapping with latitude and longitude in `python` (pg. 153)

* Coefficients of a model represent all else being equal estimates of the impacts of each feature

* How would you use an *F test* to perform more complex analysis of model coefficients?

* *Support vector machines* maximize the distance to the nearest point in each class

* *Entropy* capture the notion of how much information is in the data. We use to represent the uncertainty associated with data.

* *Bias* vs *variance*: models with high bias make a lot of mistakes that are similar across samples from the distribution and models with low variance mean that they tend to have similar parameters regardless of the sample they are trained on

* Can you identify markets using clustering?

* Could you create a recommender / matching system that given a set of products and a set of markets could perform some form of optimal matching on them?

## Datasets

* www.data.gov

* www.reddit.com/r/data

* www.reddit.com/r/datasets

* aws.amazon.com/public-data-sets

* www.kaggle.com